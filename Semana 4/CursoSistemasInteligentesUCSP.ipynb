{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('../Semana 3/shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se modifico la funcion tokenizer para que funcione de mejor forma con word2vect y asi acotar el resultado del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "del stop[116]\n",
    "del stop[116]\n",
    "del stop[116]\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()).replace('-', '')\n",
    "    text = re.sub('[\\d]+', ' ', text.lower()).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('123 This :) is a <a> test! :-)</br> . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "# Exercise 1: define features based on word embeddings (pre-trained word2vec / Glove/Fastext emebddings can be used)\n",
    "# Define suitable d dimension, and sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener Nuevas Caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso usaremos la libreria de Gensim, word2vect para obtener los vectores de cada palabra, usando como base 100 valores en cada vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para agilizar el proceso vamos a guardar en .csv los resultado obtenidos de aplicar la funcion de tokenizer sobre los review de la peliculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_words = df['review'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [teenager, martha, moxley, maggie, grace, move...\n",
      "1        [ok, really, like, kris, kristofferson, usual,...\n",
      "2        [spoiler, not, read, think, watching, movie, a...\n",
      "3        [hi, people, seen, wonderful, movie, im, sure,...\n",
      "4        [recently, bought, dvd, forgetting, much, hate...\n",
      "5        [leave, braik, put, good, show, finally, zorak...\n",
      "6        [nathan, detroit, frank, sinatra, manager, new...\n",
      "7        [understand, crash, course, right, context, mu...\n",
      "8        [impressed, chavez, stance, globalisation, som...\n",
      "9        [movie, directed, renny, harlin, finnish, mira...\n",
      "10       [lived, u, p, let, tell, foggyest, idea, heck,...\n",
      "11       [hidden, frontier, notable, longest, running, ...\n",
      "12       [ago, seen, sleuth, two, great, actors, michae...\n",
      "13       [french, first, apparently, like, jerry, lewis...\n",
      "14       [strange, movie, unlike, anything, made, west,...\n",
      "15       [saw, movie, strength, single, positive, revie...\n",
      "16       [great, philosophical, questions, purpose, lif...\n",
      "17       [cast, surfer, dude, beach, scenes, almost, go...\n",
      "18       [high, hopes, one, changed, name, shepherd, bo...\n",
      "19       [set, near, poor, working, class, town, mounta...\n",
      "20       [opulent, sets, sumptuous, costumes, well, pho...\n",
      "21       [saw, film, got, screwed, film, foolish, borin...\n",
      "22       [getting, little, tired, people, misusing, god...\n",
      "23       [offensive, liked, movie, probably, never, ope...\n",
      "24       [else, say, movie, except, plain, awful, tina,...\n",
      "25       [certain, aspects, punishment, park, less, per...\n",
      "26       [first, like, tell, comics, anime, animation, ...\n",
      "27       [not, take, say, lightly, seen, many, many, fi...\n",
      "28       [love, jurassic, park, movies, three, time, fa...\n",
      "29       [first, series, lost, kicked, bang, literally,...\n",
      "                               ...                        \n",
      "49970    [tom, fontana, unforgettable, oz, hands, one, ...\n",
      "49971    [last, weekend, bought, zombie, movie, bargain...\n",
      "49972    [watched, first, moments, tcm, years, ago, sto...\n",
      "49973    [saw, movie, first, time, hbo, loved, hilariou...\n",
      "49974    [al, pacino, kim, basinger, tea, leoni, ryan, ...\n",
      "49975    [stanwyck, villainous, best, robinson, equal, ...\n",
      "49976    [allegation, aggravated, sexual, assault, alon...\n",
      "49977    [thought, movie, wonderfully, plotted, made, c...\n",
      "49978    [like, people, wait, see, ocean, sequel, reall...\n",
      "49979    [dark, comedy, gallows, humor, one, make, come...\n",
      "49980    [probably, contain, spoilers, successful, atte...\n",
      "49981    [must, one, guy, america, like, movie, guess, ...\n",
      "49982    [plot, trucker, kristofferson, battles, corrup...\n",
      "49983    [ladies, man, laugh, loud, funny, great, diver...\n",
      "49984    [well, artyfartyrati, cannes, may, liked, film...\n",
      "49985    [director, probably, still, early, learning, s...\n",
      "49986    [know, bus, someone, decides, tell, life, stor...\n",
      "49987    [five, minutes, movie, realize, seen, boiler, ...\n",
      "49988    [laughing, much, long, time, need, take, break...\n",
      "49989    [love, dissing, movie, peers, always, try, bes...\n",
      "49990    [ok, think, tv, show, kind, cute, always, kind...\n",
      "49991    [big, disappointment, clash, night, much, talk...\n",
      "49992    [cassidy, kacia, brady, puts, gun, mouth, blow...\n",
      "49993    [rapid, intercutting, scenes, insane, people, ...\n",
      "49994    [girlfight, came, reviews, praised, get, aroun...\n",
      "49995    [ok, lets, start, best, building, although, ha...\n",
      "49996    [british, heritage, film, industry, control, n...\n",
      "49997    [even, know, begin, one, family, worst, line, ...\n",
      "49998    [richard, tyler, little, boy, scared, everythi...\n",
      "49999    [waited, long, watch, movie, also, like, bruce...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(token_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener los vectores de palabras, enviamos todas las palabra del review proprocesadas a la función Word2Vec, al no especificar un size, este se establece en 100, y guardamos el modelo para utilizarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(token_words, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo final podemos ver que se tiene un vocabulario de 101487 palabras, esto representa una gran cantidad de entradas para la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=101487, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar algunas pruebas podemos entrenar el modelo con la palabras y ver como obtenemos similaridad entre 2 vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57290522, 59944050)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(token_words, total_examples=len(token_words), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver obtenemos resultados semanticos de una palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grandmother', 0.745561957359314),\n",
       " ('daughter', 0.7072750926017761),\n",
       " ('sister', 0.6930811405181885),\n",
       " ('mom', 0.6729783415794373),\n",
       " ('roommate', 0.6648780107498169),\n",
       " ('envogue', 0.6546032428741455),\n",
       " ('brat', 0.6540639400482178),\n",
       " ('sabrina', 0.6520331501960754),\n",
       " ('spoiled', 0.6416873335838318),\n",
       " ('ager', 0.6410260200500488)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['teenager'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a nonlinear function for hidden layers.\n",
    "\n",
    "Define a suitable loss function for binary classification\n",
    "\n",
    "Implement the backpropagation algorithm for this structure\n",
    "\n",
    "Do not use Keras / Tensorflow /PyTorch etc. libraries\n",
    "\n",
    "Train the model using SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la mayor cantidad de palabras de los review para igualar todas las entradas a la red neuronal, obtenemos 1420 como maximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420\n"
     ]
    }
   ],
   "source": [
    "max_l = 0\n",
    "\n",
    "for token in token_words.values:\n",
    "    if max_l < len(token):\n",
    "        max_l = len(token)\n",
    "print(max_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la funcion getMean() que obtendra la media de cada vector de palabras, de esta forma podemos obtener un solo valor y usarlo como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMean(list_v):\n",
    "    sum_n = 0\n",
    "    for num in list_v:\n",
    "        sum_n += num\n",
    "    return sum_n / len(list_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar operaciones con mayor velocidad convertimos los datos a numpy arrays y los formateamos de forma que podamos usarlo en la red Neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in token_words:\n",
    "    temp = []\n",
    "    for wd in token:\n",
    "        temp.append(getMean(model.wv.__getitem__(wd)))\n",
    "    for idx in range(max_l - len(token)):\n",
    "        temp.append(0)\n",
    "    temp_arr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = []\n",
    "for idx, tmp in enumerate(temp_arr):\n",
    "    t = []\n",
    "    t.append(tmp)\n",
    "    t.append([df['sentiment'][idx]])\n",
    "    arr_data.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de la definición de la red Neuronal, terminamos de dar formato a nuestro dataSet, de forma que podamos usarlo con nuestra red definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_x = np.array(temp_arr)  \n",
    "\n",
    "y = df['sentiment'].values\n",
    "y_t = []\n",
    "for idx, i in enumerate(y):\n",
    "    y_t.append([y[idx]])\n",
    "np_y = np.array(y_t) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En np_x y np_y, guardamos los valores a ser entrenados, pero para realizar la validación separaremos esta data en 2 grupos con train_test_split, con el que usarmos el 80% de la data en entrenamiento y el otro 20% en la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000, 2)\n",
      "((33500, 1420), (33500, 2))\n",
      "((16500, 1420), (16500, 2))\n"
     ]
    }
   ],
   "source": [
    "y_binary = to_categorical(df['sentiment'].values)\n",
    "print(Y.shape)\n",
    "print(y_binary.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(np_x,y_binary, test_size = 0.2)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatures = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1420, 128)         256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 1420, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 650,754\n",
      "Trainable params: 650,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(fatures, 128,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " 2256/33500 [=>............................] - ETA: 3:05:58 - loss: 0.6936"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 24\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc accuracy (validation) with 1500\n",
    "val_size = 1500\n",
    "X_validate = X_test[-val_size:]\n",
    "Y_validate = Y_test[-val_size:]\n",
    "X_test = X_test[:-val_size]\n",
    "Y_test = Y_test[:-val_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un accuracy de : 51% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltas = nn.get_deltas()\n",
    "valores=[]\n",
    "index=0\n",
    "for arreglo in deltas:\n",
    "    valores.append(arreglo[0] + arreglo[1])\n",
    "    index=index+1\n",
    " \n",
    "plt.plot(range(len(valores)), valores, color='b')\n",
    "plt.ylim([0, 0.6])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y en el grafico de como obtenemos los deltas que son los errores, vemos como baja de 0.3 a 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
